# Obfuscode: Exploring the Limitations of Code Generation using Large Language Models

This repository considers methods for code obfuscation and explores their effect on an LLM's accuracy in a code generation benchmark. On a dataset of over 1900 tasks, it shows that simply renaming the variables used by the programming tasks can decrease benchmark performance by 60%.

Code and analysis can be found in the [obfuscode notebook](obfuscode.ipynb).
