# Obfuscode: Exploring the Limitations of Code Generation using Large Language Models

This notebook develops two simple transforms designed to obfuscate Python code and explores their effect on an LLM's accuracy in a code generation benchmark. On a dataset of over 1900 tasks, it shows that simply renaming the variables used in the programming tasks can decrease benchmark performance by an odds-ratio of 1.6.

Code an analysis can be found in the [obfuscode notebook](obfuscode.ipynb).
